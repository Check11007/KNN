# MA
### KNN
kNN -- k Nearest Neighbor  — один из самых простых алгоритмов классификации, также иногда используемый в задачах регрессии. Часто является алгоритмом с которого начинают знакомство с областью Machine Learning. В основном используется для классификации прогнозирующих проблем в промышленности, хотя и может применяться в более широком спектре задач. Также называется  алгоритмом ленивого обучения, потому что он не имеет специальной фазы обучения и использует все данные для обучения во время классификации. Данный алгоритм классификации относит классифицируемый объект z к тому классу y, к которому относится большинство из k его ближайших соседей. Имеется некоторая выборка , состоящая из объектов x(i), i = 1, ..., l (например, выборка ирисов Фишера), и класифицируемый объект, который обозначим z. Чтобы посчитать расстояние от классифицируемого объекта до остальных точек, нужно использовать функцию расстояния(Евклидово расстояние). Далее отсортируем объекты согласно посчитаного расстояния до объекта z:



```
sortObjectsByDist <- function(xl, z, metricFunction =
                                  euclideanDistance)
{
  l <- dim(xl)[1]
  n <- dim(xl)[2] - 1

  distances <- matrix(NA, l, 2)
  
  for (i in 1:l)
  {
    distances[i, ] <- c(i, metricFunction(xl[i, 1:n], z))
  }
  

  orderedXl <- xl[order(distances[, 2]), ]
  
  return (orderedXl);
}
```

Применение  метода kNN. Создаем функцию, которая сортирует выборку согласно нашего классифицируемого объекта z и относит его к классу ближайших соседей:

```
KNN <- function(xl, z,k)
{
  
  n <- dim(xl)[2] - 1
  
  
  classes <- xl[1:k, n + 1]

  counts <- table(classes)
  class <- names(which.max(counts))
  
  return (class)
}




### **Скользящий контроль(leave-one-out)**

LeaveOneOut (или LOO) - простая перекрестная проверка, которая необходима, чтобы оценить при каких значениях k алгоритм knn оптимален, и на сколько он ошибается. Фиксируется некоторое множество разбиений исходной выборки на две подвыборки: обучающую и контрольную. Для каждого разбиения выполняется настройка алгоритма по обучающей подвыборке, затем оценивается его средняя ошибка на объектах контрольной подвыборки. Оценкой скользящего контроля называется средняя по всем разбиениям величина ошибки на контрольных подвыборках. Другими словами, необходимо произвести проверку, как часто будет ошибаться алгоритм, если по одному выбирать элементы из обучающей выборки. Алгоритм состоит в следующем: извлечь элемент, обучить оставшиеся элементы, классифицировать извлеченный, затем вернуть его обратно. Так нужно поступить со всеми элементами выборки. Далее следует реализация на языке R:

```
LOO <- function(xl)
{
  n <- dim(xl)[1]
  iss <- matrix(0,n,1)
  for(i in 1:n){
    arg <- xl
    
    point <- arg[i,1:2]
    
   arg <- arg[-i]
    
    
    for(k in 1:(n-1)){
      class <- kNN(arg,point,k)
      if(class != iris[i,5]){
        iss[k] <- iss[k]+1
       
      }
    }
   print(i)
  }
  return(which.min(iss))
  
}
```

**Пример работы алгоритма**
![LOO](https://user-images.githubusercontent.com/71484408/96198070-72a8b900-0f5c-11eb-98bc-459dc5af5099.png)

```
